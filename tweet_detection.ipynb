{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import DistilBertPreTrainedModel, DistilBertModel, AutoTokenizer, AutoTokenizer, TrainingArguments, Trainer, AutoModelForSequenceClassification\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from torchsummary import summary\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_path(base_path, file_name):\n",
    "    return os.path.join(base_path, file_name)\n",
    "\n",
    "\n",
    "base_path = get_path(os.getcwd(), \"data\")\n",
    "train_path = get_path(base_path, \"train.csv\")\n",
    "test_path = get_path(base_path, \"test.csv\")\n",
    "\n",
    "##\n",
    "train_df = pd.read_csv(train_path)\n",
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CustomDistilBertForClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classification_head.bias', 'classification_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "class CustomDistilBertForClassification(DistilBertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super(CustomDistilBertForClassification, self).__init__(config)\n",
    "        self.num_classes = 2\n",
    "        self.distilbert = DistilBertModel(config)\n",
    "        self.classification_head = nn.Linear(config.hidden_size, self.num_classes)\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, labels=None):\n",
    "        outputs = self.distilbert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = self.classification_head(outputs.last_hidden_state[:, 0, :])  # Assuming you want to use the [CLS] token representation\n",
    "\n",
    "        if labels is not None:\n",
    "            # Apply softmax to logits before computing cross-entropy loss\n",
    "            logits = F.log_softmax(logits, dim=-1)\n",
    "            loss_fct = nn.NLLLoss()  # Negative log likelihood loss for log probabilities\n",
    "            loss = loss_fct(logits, labels)\n",
    "\n",
    "            # Ensure the loss is a scalar tensor\n",
    "            loss = loss.unsqueeze(0).mean()\n",
    "\n",
    "            return {\"loss\": loss}\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "##\n",
    "model = CustomDistilBertForClassification.from_pretrained(\"distilbert-base-uncased\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  target\n",
      "0  Our Deeds are the Reason of this #earthquake M...       1\n"
     ]
    }
   ],
   "source": [
    "train_df_ = train_df[[\"text\", \"target\"]]\n",
    "print(train_df_.head(1))\n",
    "tokens = tokenizer(train_df_[\"text\"].tolist(), return_tensors=\"pt\", padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: This is an example sentence.\n",
      "Input IDs: [101, 2023, 2003, 2019, 2742, 6251, 1012, 102]\n"
     ]
    }
   ],
   "source": [
    "text = \"This is an example sentence.\"\n",
    "input_ids = tokenizer.encode(text, add_special_tokens=True)\n",
    "\n",
    "print(\"Text:\", text)\n",
    "print(\"Input IDs:\", input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_df_[\"text\"].tolist(),\n",
    "    train_df_[\"target\"].tolist(),\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(X_train, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "val_encodings = tokenizer(X_val, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "# train_dataset = torch.utils.data.TensorDataset(\n",
    "#     train_encodings[\"input_ids\"],\n",
    "#     train_encodings[\"attention_mask\"],\n",
    "#     torch.tensor(y_train)\n",
    "# )\n",
    "\n",
    "# val_dataset = torch.utils.data.TensorDataset(\n",
    "#     val_encodings[\"input_ids\"],\n",
    "#     val_encodings[\"attention_mask\"],\n",
    "#     torch.tensor(y_val)\n",
    "# )\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, input_ids, attention_mask, labels):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_mask = attention_mask\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.input_ids[idx],\n",
    "            \"attention_mask\": self.attention_mask[idx],\n",
    "            \"labels\": self.labels[idx]\n",
    "        }\n",
    "\n",
    "# Example usage:\n",
    "train_dataset = CustomDataset(train_encodings[\"input_ids\"], train_encodings[\"attention_mask\"], y_train)\n",
    "val_dataset = CustomDataset(val_encodings[\"input_ids\"], val_encodings[\"attention_mask\"], y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                        \n",
      "  0%|          | 0/2286 [04:10<?, ?it/s]          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4663, 'learning_rate': 3.906386701662293e-05, 'epoch': 0.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                        \n",
      "\n",
      "\u001b[A\u001b[A                                           \n",
      "  0%|          | 0/2286 [04:15<?, ?it/s]          \n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.43584728240966797, 'eval_runtime': 5.2646, 'eval_samples_per_second': 289.29, 'eval_steps_per_second': 36.28, 'epoch': 0.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                        \n",
      "  0%|          | 0/2286 [05:43<?, ?it/s]           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3635, 'learning_rate': 2.8127734033245845e-05, 'epoch': 1.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                        \n",
      "\n",
      "\u001b[A\u001b[A                                           \n",
      "  0%|          | 0/2286 [05:49<?, ?it/s]           \n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4847157895565033, 'eval_runtime': 5.5334, 'eval_samples_per_second': 275.236, 'eval_steps_per_second': 34.517, 'epoch': 1.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                        \n",
      "  0%|          | 0/2286 [07:18<?, ?it/s]           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3092, 'learning_rate': 1.7191601049868766e-05, 'epoch': 1.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                        \n",
      "\n",
      "\u001b[A\u001b[A                                           \n",
      "  0%|          | 0/2286 [07:23<?, ?it/s]           \n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6416316032409668, 'eval_runtime': 5.2839, 'eval_samples_per_second': 288.233, 'eval_steps_per_second': 36.147, 'epoch': 1.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                        \n",
      "  0%|          | 0/2286 [08:45<?, ?it/s]           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1848, 'learning_rate': 6.255468066491689e-06, 'epoch': 2.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                        \n",
      "\n",
      "\u001b[A\u001b[A                                           \n",
      "  0%|          | 0/2286 [08:53<?, ?it/s]           \n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.73227858543396, 'eval_runtime': 7.5986, 'eval_samples_per_second': 200.431, 'eval_steps_per_second': 25.136, 'epoch': 2.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                        \n",
      "100%|██████████| 2286/2286 [07:03<00:00,  5.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 423.7543, 'train_samples_per_second': 43.115, 'train_steps_per_second': 5.395, 'train_loss': 0.3118278923935778, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2286, training_loss=0.3118278923935778, metrics={'train_runtime': 423.7543, 'train_samples_per_second': 43.115, 'train_steps_per_second': 5.395, 'train_loss': 0.3118278923935778, 'epoch': 3.0})"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./distilbert-finetuned\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_total_limit=2,\n",
    "    eval_steps=500,\n",
    "    logging_steps=500,\n",
    "    learning_rate=5e-5,\n",
    "    save_steps=500,\n",
    ")\n",
    "\n",
    "# Instantiate Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomDistilBertForClassification(\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0): TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (1): TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (2): TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (3): TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (4): TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (5): TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classification_head): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 191/191 [00:03<00:00, 56.90it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=(), label_ids=array([1, 0, 1, ..., 1, 1, 0], dtype=int64), metrics={'test_loss': 0.7472735047340393, 'test_runtime': 3.3731, 'test_samples_per_second': 451.52, 'test_steps_per_second': 56.625})"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use Trainer.predict \n",
    "predictions = trainer.predict(val_dataset)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Validation Loss: 0.5324012882386645\n",
      "Epoch 2/3, Validation Loss: 0.7892291077102224\n",
      "Epoch 3/3, Validation Loss: 0.753568139548103\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "# Training parameters\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "num_epochs = 3\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs[\"loss\"]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            val_losses.append(outputs[\"loss\"].item())\n",
    "\n",
    "    avg_val_loss = sum(val_losses) / len(val_losses)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Validation Loss: {avg_val_loss}\")\n",
    "\n",
    "# Save the trained model if needed\n",
    "torch.save(model.state_dict(), \"custom_distilbert_classification_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# model_p = CustomDistilBertForClassification.from_pretrained(\"custom_distilbert_classification_model.pth\")\n",
    "model_p = model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Move the model to the appropriate device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_p.to(device)\n",
    "\n",
    "# Load validation dataset\n",
    "val_encodings = tokenizer(X_val, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "val_dataset = CustomDataset(val_encodings[\"input_ids\"], val_encodings[\"attention_mask\"], y_val)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Make predictions on the validation dataset\n",
    "model_p.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "        outputs = model_p(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.squeeze()  # Assuming your logits are in the shape (batch_size, num_classes)\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(batch[\"labels\"].cpu().numpy())\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "\n",
    "# # Plot confusion matrix\n",
    "# classes = ['Class 0', 'Class 1']  # Modify based on your actual class names\n",
    "# disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
    "# disp.plot(cmap=plt.cm.Blues, values_format=\".4g\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.83      0.84      0.84       874\n",
      "    Positive       0.78      0.78      0.78       649\n",
      "\n",
      "    accuracy                           0.81      1523\n",
      "   macro avg       0.81      0.81      0.81      1523\n",
      "weighted avg       0.81      0.81      0.81      1523\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(all_labels, all_preds, target_names=[\"Negative\", \"Positive\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.save_model(\"./distilbert-finetuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CustomDistilBertForClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classification_head.bias', 'classification_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CustomDistilBertForClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classification_head): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model = CustomDistilBertForClassification.from_pretrained('distilbert-base-uncased')\n",
    "loaded_model.load_state_dict(torch.load(\"custom_distilbert_classification_model.pth\"))\n",
    "\n",
    "# Move the model to the appropriate device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "loaded_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_to_load = 2000\n",
    "best_model =  CustomDistilBertForClassification.from_pretrained(f\"./distilbert-finetuned/checkpoint-{checkpoint_to_load}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                text\n",
       "0   0     NaN      NaN  Just happened a terrible car crash"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(test_path)\n",
    "test_df.head(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
